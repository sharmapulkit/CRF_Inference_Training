{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import itertools\n",
    "from scipy.special import logsumexp\n",
    "import scipy.optimize as sciop\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_gradient = pd.read_csv(PATH + 'model/transition-gradient.txt', delimiter=' ', header=None)\n",
    "transition_params = pd.read_csv(PATH + 'model/transition-params.txt', delimiter=' ', header=None)\n",
    "feature_gradient = pd.read_csv(PATH + 'model/feature-gradient.txt', delimiter=' ', header=None)\n",
    "feature_params = pd.read_csv(PATH + 'model/feature-params.txt', delimiter=' ', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = open(PATH + 'data/train_words.txt').read().splitlines()\n",
    "test_labels = open(PATH + 'data/test_words.txt').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllChars = \"etainoshrd\"\n",
    "CharMapping = \\\n",
    "{'e': 0,\n",
    " 't': 1,\n",
    " 'a': 2,\n",
    " 'i': 3,\n",
    " 'n': 4,\n",
    " 'o': 5,\n",
    " 's': 6,\n",
    " 'h': 7,\n",
    " 'r': 8,\n",
    " 'd': 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.4e}'.format)\n",
    "# pd.reset_option('display.float_format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePotentials(wordFeats, featureParams):\n",
    "    phis = []\n",
    "    for letter_id in range(len(wordFeats)):\n",
    "        phis.append(np.sum(np.multiply(featureParams, wordFeats[letter_id, :]), axis=1))\n",
    "    return np.array(phis).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeEnergy(wordFeats, wordLabel, Potentials, transitionParams):\n",
    "    wordLabel_CharIdx = [CharMapping[x] for x in wordLabel]\n",
    "    sum_phis = np.sum(Potentials[wordLabel_CharIdx, range(0, len(wordFeats))])\n",
    "    \n",
    "    arr = [(x, y) for (x, y) in zip(wordLabel_CharIdx[:-1], wordLabel_CharIdx[1:])]\n",
    "    transition_param_sum = 0\n",
    "    for elem in arr:\n",
    "        transition_param_sum += transitionParams.iloc[elem]\n",
    "    return transition_param_sum + sum_phis\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Nodes are 1 indexed\n",
    "def messageDict(word, featParams, transParams, wordPhis=None):\n",
    "    \"\"\" Returns Messages from one node to another Dictionaries in log-space \"\"\"\n",
    "    ########### Compute Word Potentials #########\n",
    "    if wordPhis is None:\n",
    "        wordPhis = computePotentials(word, featParams)\n",
    "        \n",
    "    logM = dict()\n",
    "    logM[len(word) + 1, len(word)] = np.zeros(len(AllChars))\n",
    "    logM[0, 1] = np.zeros(len(AllChars))\n",
    "    \n",
    "    #### Backward Messages ####\n",
    "    for fromNode in range(len(word), 1, -1):\n",
    "        logM[fromNode, fromNode - 1] = logsumexp(wordPhis[:, fromNode-1] + \\\n",
    "                              logM[fromNode + 1, fromNode] + \\\n",
    "                              transParams.values, axis = 1)    \n",
    "        \n",
    "    #### Forward Messages ####\n",
    "    for fromNode in range(1, len(word)):\n",
    "        logM[fromNode, fromNode + 1] = logsumexp(wordPhis[:, fromNode-1] + \\\n",
    "                              logM[fromNode - 1, fromNode] + \\\n",
    "                                transParams.values, axis=1)    \n",
    "    return logM\n",
    "\n",
    "def singleVariableMarginal(node, word, featParams, transParams, wordPhis=None, logPartition=None):\n",
    "    if wordPhis is None:\n",
    "        wordPhis = computePotentials(word, featParams)\n",
    "    \n",
    "    _messageDict = messageDict(word, featParams, transParams, wordPhis)\n",
    "    if (node > 0):\n",
    "        m_left = _messageDict[node-1, node]\n",
    "    if (node < len(word)+1):\n",
    "        m_right = _messageDict[node+1, node]\n",
    "    \n",
    "    total_incoming_message = m_left + m_right\n",
    "    \n",
    "    ############\n",
    "    if (logPartition is None):\n",
    "        logPartition = logsumexp(wordPhis[:, node-1] + total_incoming_message)\n",
    "        \n",
    "    logMarginalProbs = wordPhis[:, node-1] + total_incoming_message - logPartition\n",
    "    return np.exp(logMarginalProbs)\n",
    "\n",
    "def pairwiseMarginal(nodeL, nodeR, word, featParams, transParams, logPartition=None):\n",
    "    \"\"\" PairWise Marginals \"\"\"\n",
    "    wordPhis = computePotentials(word, featParams)\n",
    "    _messageDict = messageDict(word, featParams, transParams, wordPhis)\n",
    "    \n",
    "    if (logPartition is None):\n",
    "        logPartition = logsumexp(wordPhis[:, 0] + _messageDict[2, 1])\n",
    "        \n",
    "    chars_to_consider = AllChars\n",
    "    pairwise_marginals = np.zeros( (len(chars_to_consider), len(chars_to_consider)) )\n",
    "    \n",
    "    mLeft = 0\n",
    "    mRight = 0\n",
    "    if (nodeL >= 1):\n",
    "        mLeft = _messageDict[nodeL - 1, nodeL]\n",
    "    if (nodeR <= len(word)):\n",
    "        mRight = _messageDict[nodeR + 1, nodeR]\n",
    "        \n",
    "    logSingleMarginalsL = wordPhis[:, nodeL-1] + mLeft\n",
    "    logSingleMarginalsL = np.stack( [logSingleMarginalsL]*len(AllChars) ).T\n",
    "    logSingleMarginalsR = wordPhis[:, nodeR-1] + mRight\n",
    "    logSingleMarginalsR = np.stack( [logSingleMarginalsR]*len(AllChars) )\n",
    "    \n",
    "    SingleMarginals = logSingleMarginalsL + logSingleMarginalsR\n",
    "    jointMarginal = SingleMarginals + transParams - logPartition\n",
    "\n",
    "    return np.exp(jointMarginal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "['trat', 'hire', 'riser', 'edison', 'shore', 'tenth', 'hot', 'tests', 'trains', 'order', 'taare', 'rose', 'roton', 'ihtention', 'shots', 'starts', 'andee', 'rhinr', 'retained', 'the', 'hehderson', 'rise', 'either', 'read', 'insisted', 'theatoe', 'tao', 'shot', 'eitstein', 'tne', 'roe', 'tension', 'restored', 'donated', 'don', 'entries', 'attesied', 'iron', 'session', 'heirs', 'need', 'hair', 'rather', 'tree', 'initiate', 'stressed', 'dish', 'dore', 'oath', 'estate', 'adoitton', 'norrneastern', 'arid', 'sins', 'area', 'and', 'nad', 'ann', 'nose', 'shnre', 'threateneo', 'sead', 'edtth', 'antonio', 'thiro', 'tender', 'rosd', 'sent', 'trehd', 'traded', 'rated', 'iit', 'resadent', 'resisthni', 'hero', 'anne', 'adoeroids', 'ten', 'raoio', 'strerts', 'serena', 'statd', 'thtoat', 'ordeas', 'noted', 'ardse', 'reridted', 'deied', 'serds', 'oas', 'sri', 'thnt', 'ratio', 'threat', 'noise', 'tet', 'sin', 'riots', 'sanitation', 'are', 'net', 'neao', 'oset', 'haa', 'assertton', 'theories', 'and', 'thirreenth', 'dnto', 'rio', 'hoetea', 'titan', 'treated', 'and', 'aaror', 'this', 'does', 'atheniahs', 'dorset', 'iaoia', 'harsh', 'soot', 'reasons', 'tornh', 'eates', 'arsser', 'stariored', 'anton', 'naa', 'that', 'oad', 'nations', 'retire', 'naiti', 'trena', 'added', 'and', 'rnd', 'tornaoo', 'ratiot', 'densatior', 'states', 'soor', 'hands', 'asd', 'shorteneh', 'denird', 'das', 'other', 'rains', 'attend', 'stanoard', 'addis', 'retreat', 'areas', 'riaho', 'inret', 'hanoi', 'than', 'that', 'oae', 'tree', 'theatres', 'theater', 'atstoria', 'raises', 'atrention', 'narrhtot', 'strait', 'northeast', 'interested', 'thii', 'henri', 'rada', 'ido', 'distant', 'hihtrndo', 'rano', 'root', 'resided', 'heart', 'sarine', 'stern', 'trait', 'trinidad', 'erao', 'ooae', 'hdrse', 'send', 'sort', 'ohdered', 'tintdn', 'and', 'sheer', 'irnn', 'deteriorated', 'others', 'notta', 'restd', 'hor']\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for testWordIdx in range(1, 201):\n",
    "    testWord = pd.read_csv(PATH + 'data/test_img{}.txt'.format(testWordIdx), header=None, delimiter=\" \")\n",
    "    testWord = testWord.values\n",
    "    \n",
    "    testWordPhis = computePotentials(testWord, feature_params)\n",
    "    singleVarMarginals = []\n",
    "    for node in range(1, len(testWord)+1):\n",
    "        singleVarMarginals.append(singleVariableMarginal(node, testWord, feature_params, transition_params, \\\n",
    "                                                         testWordPhis))\n",
    "    singleVarMarginals = np.array(singleVarMarginals)\n",
    "#     print([AllChars[x] for x in np.argmax(singleVarMarginals, axis=1)])\n",
    "    preds.append(''.join([AllChars[x] for x in np.argmax(singleVarMarginals, axis=1)]))\n",
    "\n",
    "print(\"Predictions:\")\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Write predictions to a file ############\n",
    "predictionsOutfile23 = \"predictionsOutfile23.txt\"\n",
    "with open(predictionsOutfile23, 'w') as f:\n",
    "    for pred in preds:\n",
    "        predString = pred\n",
    "        f.write(predString)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Level Test-Accuracy: 89.91674375578168 %\n"
     ]
    }
   ],
   "source": [
    "#### Character Level accuracy ########\n",
    "tp = 0\n",
    "total = 0\n",
    "for pred_id, pred in enumerate(preds):\n",
    "    testWordLabel = np.array([x for x in test_labels[pred_id]])\n",
    "    pred = np.array([x for x in pred])\n",
    "    tp += len(np.where(testWordLabel == pred)[0])\n",
    "    total += len(testWordLabel)\n",
    "\n",
    "print(\"Character Level Test-Accuracy:\", tp/total*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Compute Log Likelihood ######\n",
    "def loglikelihood(W, dataSize):\n",
    "    featParams, transParams = W[:len(AllChars)*featSize], W[len(AllChars)*featSize:]\n",
    "    featParams = pd.DataFrame(np.reshape(featParams, (len(AllChars), featSize)))\n",
    "    transParams = pd.DataFrame(np.reshape(transParams, (len(AllChars), len(AllChars))))\n",
    "    loglikelihood = 0\n",
    "    for trainWordIdx in range(0, dataSize):\n",
    "        trainWord = pd.read_csv(PATH + 'data/train_img{}.txt'.format(trainWordIdx+1), header=None, delimiter=\" \")\n",
    "        trainWord = trainWord.values\n",
    "        trainWord_label = train_labels[trainWordIdx]\n",
    "        trainWord_labelIdxs = [CharMapping[x] for x in trainWord_label]\n",
    "\n",
    "        trainWordPhis = computePotentials(trainWord, featParams)\n",
    "        trainWordlogM = messageDict(trainWord, featParams, transParams)\n",
    "        marginal = trainWordPhis[:, 0] + trainWordlogM[(2, 1)]\n",
    "        logZ = logsumexp(marginal)\n",
    "    #     print(logZ)\n",
    "\n",
    "        trainWordEnergy = computeEnergy(trainWord, trainWord_label, trainWordPhis, transParams)\n",
    "    #     print(trainWordEnergy)\n",
    "        loglikelihood += trainWordEnergy - logZ\n",
    "    return loglikelihood/dataSize\n",
    "\n",
    "    print(\"Log likelihood for first {} train data points:\".format(dataSize), loglikelihood/dataSize )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood for first 50 train data points: -4.583959036355723\n"
     ]
    }
   ],
   "source": [
    "avglll = loglikelihood((feature_params, transition_params), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _computeGradWF(word, wordLabel, singleMarginals):\n",
    "    wordLabelList = np.array([x for x in wordLabel])\n",
    "    mask = [np.where(wordLabelList == c)[0] for c in CharMapping] # 10 x L(word)\n",
    "    selectedX = [np.sum(word[m, :], axis=0) for m in mask] # 10 x _ x 321\n",
    "\n",
    "    secondTerm = []\n",
    "    for c_id, c in enumerate(AllChars):\n",
    "        probbyc = np.expand_dims(singleMarginals[:, c_id], axis=1)\n",
    "        secondTerm.append(np.sum(word*probbyc, axis=0))\n",
    "    secondTerm = np.array(secondTerm)\n",
    "    grad = selectedX - secondTerm\n",
    "    return grad\n",
    "\n",
    "def computeGradF(W, dataSize):\n",
    "    wF, wT = W[:len(AllChars)*featSize], W[len(AllChars)*featSize:]\n",
    "    wF = pd.DataFrame(np.reshape(wF, (len(AllChars), featSize)))\n",
    "    wT = pd.DataFrame(np.reshape(wT, (len(AllChars), len(AllChars))))\n",
    "    grad = 0\n",
    "    for trainWordIdx in range(dataSize):\n",
    "        trainWord = pd.read_csv(PATH + 'data/train_img{}.txt'.format(trainWordIdx+1), header=None, delimiter=\" \")\n",
    "        trainWord = trainWord.values\n",
    "        trainWord_label = train_labels[trainWordIdx]\n",
    "        trainWord_labelIdxs = [CharMapping[x] for x in trainWord_label]\n",
    "        trainWordPhis = computePotentials(trainWord, wF)\n",
    "\n",
    "        singleVarMargs = []\n",
    "        for node in range(1, len(trainWord)+1):\n",
    "            sVM = singleVariableMarginal(node, trainWord, wF, wT, trainWordPhis)\n",
    "            singleVarMargs.append(sVM)\n",
    "        grad += _computeGradWF(trainWord, trainWord_label, np.array(singleVarMargs))\n",
    "    return grad/dataSize\n",
    "\n",
    "def _computeGradWT(word, wordLabel, _pairwiseMarginals):\n",
    "    gradwt = np.zeros((len(AllChars), len(AllChars)))\n",
    "    for c in CharMapping:\n",
    "        for cprime in CharMapping:\n",
    "            firstTerm = 0\n",
    "            secondTerm = 0\n",
    "            for letter_id in range(0, len(word)-1):\n",
    "                if ((wordLabel[letter_id] == c) and (wordLabel[letter_id + 1] == cprime)):\n",
    "                    firstTerm += 1\n",
    "                secondTerm += _pairwiseMarginals[letter_id][CharMapping[c]][CharMapping[cprime]]\n",
    "            gradwt[CharMapping[c], CharMapping[cprime]] = (firstTerm - secondTerm)\n",
    "    return gradwt\n",
    "\n",
    "def computeGradT(W, dataSize):\n",
    "    wF, wT = W[:len(AllChars)*featSize], W[len(AllChars)*featSize:]\n",
    "    wF = pd.DataFrame(np.reshape(wF, (len(AllChars), featSize)))\n",
    "    wT = pd.DataFrame(np.reshape(wT, (len(AllChars), len(AllChars))))\n",
    "    gradWT = 0\n",
    "    for trainWordIdx in range(dataSize):\n",
    "        trainWord = pd.read_csv(PATH + 'data/train_img{}.txt'.format(trainWordIdx+1), header=None, delimiter=\" \")\n",
    "        trainWord = trainWord.values\n",
    "        trainWord_label = train_labels[trainWordIdx]\n",
    "        trainWord_labelIdxs = [CharMapping[x] for x in trainWord_label]\n",
    "        trainWordPhis = computePotentials(trainWord, wF)\n",
    "\n",
    "        pairwiseMargs = []\n",
    "        for node in range(1, len(trainWord)):\n",
    "            pM = pairwiseMarginal(node, node+1, trainWord, wF, wT)\n",
    "            pairwiseMargs.append(pM.values)\n",
    "        gradWT += _computeGradWT(trainWord, trainWord_label, np.array(pairwiseMargs))\n",
    "    return gradWT/dataSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradlll(W, dataSize):\n",
    "    \"\"\" Compute the gradient of log likelihood at wF and wT \"\"\"\n",
    "    gradWF = computeGradF(W, dataSize)\n",
    "    gradWT = computeGradT(W, dataSize)\n",
    "\n",
    "    grads = np.concatenate( (gradWF.flatten(), gradWT.flatten()) )\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(W, dataSize):\n",
    "    return -1*loglikelihood(W, dataSize)\n",
    "\n",
    "def gradnll(W, dataSize):\n",
    "    return -1*gradlll(W, dataSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times = {}\n",
    "accuracies = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000001\n",
      "         Iterations: 19\n",
      "         Function evaluations: 56\n",
      "         Gradient evaluations: 45\n",
      "Training Time: 445.5863754749298\n"
     ]
    }
   ],
   "source": [
    "featSize = 321\n",
    "dataSize = 100\n",
    "### train with given data ###\n",
    "x0 = np.concatenate( (np.ones((len(AllChars)*featSize)), np.ones((len(AllChars)*len(AllChars)))) )\n",
    "# x0 = (np.ones((len(AllChars), featSize)), np.ones(len(AllChars), len(AllChars)))\n",
    "startTime = time.time()\n",
    "res = sciop.minimize(fun=nll, x0=x0, args=dataSize, jac=gradnll, method='BFGS', options={'disp': True, 'maxiter': 40})\n",
    "endTime = time.time()\n",
    "training_time = endTime - startTime\n",
    "print(\"Training Time:\", training_time)\n",
    "training_times[dataSize] = training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.x\n",
    "weights[dataSize] = res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(W):\n",
    "    wF, wT = W[:len(AllChars)*featSize], W[len(AllChars)*featSize:]\n",
    "    wF = pd.DataFrame(np.reshape(wF, (len(AllChars), featSize)))\n",
    "    wT = pd.DataFrame(np.reshape(wT, (len(AllChars), len(AllChars))))\n",
    "    \n",
    "    #### Predictions on test set\n",
    "    preds = []\n",
    "    for testWordIdx in range(1, 201):\n",
    "        testWord = pd.read_csv(PATH + 'data/test_img{}.txt'.format(testWordIdx), header=None, delimiter=\" \")\n",
    "        testWord = testWord.values\n",
    "\n",
    "        testWordPhis = computePotentials(testWord, wF)\n",
    "        singleVarMarginals = []\n",
    "        for node in range(1, len(testWord)+1):\n",
    "            singleVarMarginals.append(singleVariableMarginal(node, testWord, wF, wT, \\\n",
    "                                                             testWordPhis))\n",
    "        singleVarMarginals = np.array(singleVarMarginals)\n",
    "    #     print([AllChars[x] for x in np.argmax(singleVarMarginals, axis=1)])\n",
    "        preds.append(''.join([AllChars[x] for x in np.argmax(singleVarMarginals, axis=1)]))\n",
    "\n",
    "#     print(\"Predictions:\")\n",
    "#     print(preds)\n",
    "    \n",
    "    #### Character Level accuracy ########\n",
    "    tp = 0\n",
    "    total = 0\n",
    "    for pred_id, pred in enumerate(preds):\n",
    "        testWordLabel = np.array([x for x in test_labels[pred_id]])\n",
    "        pred = np.array([x for x in pred])\n",
    "        tp += len(np.where(testWordLabel == pred)[0])\n",
    "        total += len(testWordLabel)\n",
    "    \n",
    "    print(\"Character Level Test-Accuracy:\", tp/total*100, \"%\")\n",
    "    return tp/total*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Level Test-Accuracy: 91.0268270120259 %\n"
     ]
    }
   ],
   "source": [
    "acc = getAccuracy(res.x)\n",
    "accuracies[dataSize] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({50: 272.1810564994812, 100: 445.5863754749298},\n",
       " {50: None, 100: 91.0268270120259},\n",
       " 1)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_times, accuracies, len(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baad mein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _computeGradWcf(word, wordLabel, singleMarginals, c, f):\n",
    "    wordLabelList = np.array([x for x in wordLabel])\n",
    "    mask = np.where(wordLabelList == c)\n",
    "    selectedX = word[mask, f]\n",
    "    singleMar = singleMarginals[:, CharMapping[c]]\n",
    "    grad = np.sum(selectedX) - np.sum(singleMar*word[:, f])\n",
    "    return grad\n",
    "    \n",
    "def computeFeatureGrad(c, f):\n",
    "    grad = 0\n",
    "    for trainWordIdx in range(0, 50):\n",
    "        trainWord = pd.read_csv(PATH + 'data/train_img{}.txt'.format(trainWordIdx+1), header=None, delimiter=\" \")\n",
    "        trainWord = trainWord.values\n",
    "        trainWord_label = train_labels[trainWordIdx]\n",
    "        trainWord_labelIdxs = [CharMapping[x] for x in trainWord_label]\n",
    "        trainWordPhis = computePotentials(trainWord, feature_params)\n",
    "        \n",
    "        singleVarMargs = []\n",
    "        for node in range(1, len(trainWord)+1):\n",
    "            sVM = singleVariableMarginal(node, trainWord, feature_params, transition_params, trainWordPhis)\n",
    "            singleVarMargs.append(sVM)\n",
    "        thisGrad = _computeGradWcf(trainWord, trainWord_label, np.array(singleVarMargs), c, f)\n",
    "        grad += thisGrad\n",
    "    return grad\n",
    "\n",
    "def _computeGradWccp(word, wordLabel, _pairwiseMarginals, c, cprime):\n",
    "    firstTerm = 0\n",
    "    secondTerm = 0\n",
    "    for letter_id in range(0, len(word)-1):\n",
    "        if ((wordLabel[letter_id] == c) and (wordLabel[letter_id + 1] == cprime)):\n",
    "            firstTerm += 1\n",
    "        secondTerm += _pairwiseMarginals[letter_id][CharMapping[c]][CharMapping[cprime]]\n",
    "    return (firstTerm - secondTerm)\n",
    "    \n",
    "def computeTransitionGrad(c, cprime):\n",
    "    grad = 0\n",
    "    for trainWordIdx in range(0, 50):\n",
    "        trainWord = pd.read_csv(PATH + 'data/train_img{}.txt'.format(trainWordIdx+1), header=None, delimiter=\" \")\n",
    "        trainWord = trainWord.values\n",
    "        trainWord_label = train_labels[trainWordIdx]\n",
    "        trainWordPhis = computePotentials( trainWord, feature_params )\n",
    "        \n",
    "        pairwiseMargs = []\n",
    "        for node in range(1, len(trainWord)):\n",
    "            pM = pairwiseMarginal(node, node+1, trainWord, feature_params, transition_params)\n",
    "            pairwiseMargs.append(pM.values)\n",
    "        thisGrad = _computeGradWccp(trainWord, trainWord_label, np.array(pairwiseMargs), c, cprime)\n",
    "        grad += thisGrad\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(firstWordPhisDF.to_latex())\n",
    "print(df_margProbDist.to_latex())\n",
    "print(allMessages_21.to_latex())\n",
    "print(probs_pd.to_latex())\n",
    "print(jointFirstWord12.to_latex())\n",
    "print(jointFirstWord23.to_latex())\n",
    "print(jointFirstWord34.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 19\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(Z):\n",
    "    x, y = Z\n",
    "    return (((1-x)*(1-x)) + 100*((y-x*x)*(y-x*x)))\n",
    "\n",
    "def gradf(Z):\n",
    "    x, y = Z\n",
    "    return np.array([-2*(1-x) - 400*x*(y-x*x), 200*(y-x*x)])\n",
    "\n",
    "x0 = [0, 0]\n",
    "res = sciop.minimize(fun=f, x0=x0, jac=gradf, method='BFGS', options={'disp': True})\n",
    "res.x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
